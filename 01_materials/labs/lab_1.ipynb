{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Neural Networks with Keras\n",
    "\n",
    "Welcome to the first practical session of the course! In this session, we will learn how to train neural networks with Keras. We will start with a simple example of a feedforward neural network for classification and then we will study the impact of the initialization of the weights on the convergence of the training algorithm.\n",
    "\n",
    "Keras is a high-level neural network API, built on top of TensorFlow 2.0. It provides a user-friendly interface to build, train and deploy deep learning models. Keras is designed to be modular, fast and easy to use.\n",
    "\n",
    "Throughout this course, we will focus on using Keras and TensorFlow for building and training neural networks. However, there are other popular deep learning frameworks such as PyTorch, MXNet, CNTK, etc. that you can also use to build and train neural networks.\n",
    "\n",
    "In order to use our code on Google Colab, we will need to ensure that any required packages are installed. We will use the following packages in this session:\n",
    "\n",
    "- `tensorflow`: an open-source library for numerical computation and large-scale machine learning.\n",
    "- `matplotlib`: a plotting library for the Python programming language and its numerical mathematics extension NumPy.\n",
    "- `numpy`: a library for scientific computing in Python.\n",
    "- `scikit-learn`: a machine learning library for the Python programming language.\n",
    "- `pandas`: a library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language.\n",
    "\n",
    "The following cell will check if the packages are installed, and if not, it will install them. Students familiar with how pip works might note that it already checks this before installing! The reason for this code (which will also appear in subsequent notebooks) is to speed up execution if you re-run the entire notebook - it will skip the installation step if the packages are already installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow is already installed.\n",
      "matplotlib is already installed.\n",
      "numpy is already installed.\n",
      "scikit-learn is missing.\n",
      "pandas is already installed.\n",
      "\n",
      "Missing packages: ['scikit-learn']\n"
     ]
    }
   ],
   "source": [
    "# Check for missing packages\n",
    "import importlib\n",
    "\n",
    "# List of required packages\n",
    "required_packages = ['tensorflow', 'matplotlib', 'numpy', 'scikit-learn', 'pandas']\n",
    "missing_packages = []\n",
    "\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        importlib.import_module(package)\n",
    "        print(f\"{package} is already installed.\")\n",
    "    except ImportError:\n",
    "        print(f\"{package} is missing.\")\n",
    "        missing_packages.append(package)\n",
    "\n",
    "# Display missing packages\n",
    "if missing_packages:\n",
    "    print(\"\\nMissing packages:\", missing_packages)\n",
    "else:\n",
    "    print(\"\\nAll required packages are installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing missing packages...\n",
      "Requirement already satisfied: scikit-learn in /Users/nrojas/anaconda3/envs/dsi_participant/lib/python3.9/site-packages (1.5.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /Users/nrojas/anaconda3/envs/dsi_participant/lib/python3.9/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/nrojas/anaconda3/envs/dsi_participant/lib/python3.9/site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/nrojas/anaconda3/envs/dsi_participant/lib/python3.9/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/nrojas/anaconda3/envs/dsi_participant/lib/python3.9/site-packages (from scikit-learn) (3.5.0)\n",
      "\n",
      "Installation complete!\n"
     ]
    }
   ],
   "source": [
    "# Install missing packages\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "if missing_packages:\n",
    "    print(\"Installing missing packages...\")\n",
    "    for package in missing_packages:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "    print(\"\\nInstallation complete!\")\n",
    "else:\n",
    "    print(\"\\nNo packages to install.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "With the packages installed, we can now get started on the practical session!\n",
    "\n",
    "Today, we will be working with the famous MNIST dataset. MNIST (Modified National Institute of Standards and Technology) is a database of low resolution images of handwritten digits. The history here is interesting - the dataset was originally created in the 1980s, when researchers from the aforementioned institute collected samples from American Census Bureau employees and high school students. The dataset was then modified in the 1990s (hence the M in MNIST), and has since become a popular benchmark for machine learning algorithms. \n",
    "\n",
    "The dataset contains images, each of which is a 28x28 grayscale image of a handwritten digit. The goal is to classify each image into one of the 10 possible classes (0-9).\n",
    "\n",
    "![MNIST](https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png)\n",
    "\n",
    "The Scikit-Learn library provides a convenient function to download and load the MNIST dataset. The following cell will download the dataset. Then we will take a look at the shape of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 8, 8)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits.images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This means that we have 1797 images, each of which is a 8x8 image. For basic image processing, we will need to flatten the images into a 1D array. In this case, Scikit-Learn has already provided the data in this format too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 64)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "For each image, we also have the corresponding label (or target, or class) in `digits.target`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797,)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits.target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Each image is an 8x8 grayscale image. If we want to use these images for machine learning, we typically flatten the 2D arrays into 1D arrays of size 8 \\times 8 = 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images: 1797\n",
      "Shape of each image: (8, 8)\n",
      "Shape of data after flattening: (1797, 64)\n",
      "Shape of target after flattening: (1797,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "digits = load_digits()\n",
    "\n",
    "# Data and target\n",
    "X, y = digits.data, digits.target\n",
    "\n",
    "# Print shape of the dataset\n",
    "print(\"Number of images:\", len(X))\n",
    "print(\"Shape of each image:\", digits.images[0].shape)\n",
    "print(\"Shape of data after flattening:\", X.shape)\n",
    "print(\"Shape of target after flattening:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can take a look at some random images from the dataset. The following cell will select 9 random images and plot them in a 3x3 grid (meaning that you can rerun the cell to see different images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Selecting 9 random indices\n",
    "random_indices = np.random.choice(len(digits.images), 9, replace=False)\n",
    "\n",
    "# Creating a 3x3 grid plot\n",
    "fig, axes = plt.subplots(3, 3, figsize=(6, 6))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(digits.images[random_indices[i]], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    ax.set_title(f\"Label: {digits.target[random_indices[i]]}\")\n",
    "\n",
    "    # Removing axis labels\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "As you can see, these images are very low resolution. This is because they were originally scanned from paper forms, and then scaled down to 8x8 pixels. This is a common problem in machine learning - the quality of the data is often a limiting factor in the performance of the model. In this case, the low resolution of the images makes it difficult to distinguish between some digits, even for humans. For example, the following images are all labelled as 9, but they look very different:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Selecting 9 random indices of images labelled as 9\n",
    "random_indices = np.random.choice(np.where(digits.target == 9)[0], 9, replace=False)\n",
    "\n",
    "# Creating a 3x3 grid plot\n",
    "fig, axes = plt.subplots(3, 3, figsize=(6, 6))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(digits.images[random_indices[i]], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    ax.set_title(f\"Label: {digits.target[random_indices[i]]}\")\n",
    "\n",
    "    # Removing axis labels\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "While we are plotting the samples as images, remember that our model is only going to see a 1D array of numbers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / Test Split\n",
    "\n",
    "In order to understand how well our model performs on _new_ data, we need to split our dataset into a training set and a test set. The training set will be used to train the model, and the test set will be used to evaluate the performance of the model.\n",
    "\n",
    "Let's keep some held-out data to be able to measure the generalization performance of our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    digits.data, \n",
    "    digits.target,\n",
    "    test_size=0.2, # 20% of the data is used for testing\n",
    "    random_state=42, # Providing a value here means getting the same \"random\" split every time\n",
    "    stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let's confirm that the data has been split correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (1437, 64)\n",
      "y_train shape: (1437,)\n",
      "X_test shape: (360, 64)\n",
      "y_test shape: (360,)\n"
     ]
    }
   ],
   "source": [
    "print(f'X_train shape: {X_train.shape}')\n",
    "print(f'y_train shape: {y_train.shape}')\n",
    "print(f'X_test shape: {X_test.shape}')\n",
    "print(f'y_test shape: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This is what we expected to see. It's always good to check as you go, to make sure that you haven't made a mistake somewhere - this is something that working in a notebook like this makes it easy to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing of the Target Data\n",
    "\n",
    "The labels that we have are integers between 0 and 9. However, we want to train a neural network to classify the images into one of 10 classes. It can be a little counter-intuitive because we are dealing with numbers, but our classes are not ordinal.\n",
    "\n",
    "What do we mean by that? Let's imagine we were trying to predict the height of a building (separated into classes) from images. If a given building was actually 10m tall, and our model predicted 9m, we would consider that to be a better prediction than if it predicted 1m. This is because the classes are ordinal - there is meaning in the difference between the classes.\n",
    "\n",
    "In our case, even though we are dealing with numbers, the classes are not ordinal. If a given image is actually a 9, and our model predicts 8, we would consider that to be just as bad as if it predicted 1. This is because the classes are not ordered, and the difference between the classes is not meaningful.\n",
    "\n",
    "Because of this, we need to convert our labels from an integer value into a one-hot encoded vector. This means that each label will be represented as a vector of length 10, with a 1 in the position corresponding to the class, and 0s everywhere else. For example, the label 9 would be represented as `[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]`. This is a common way of representing categorical data in machine learning. By doing this, we ensure that our model is taught the correct relationship between the classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using TensorFlow/Keras\n",
    "\n",
    "TensorFlow and Keras provide a convenient method for one-hot encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before one-hot encoding: 5\n",
      "After one-hot encoding: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "print(f'Before one-hot encoding: {y_train[0]}')\n",
    "y_train = to_categorical(y_train, num_classes=10)\n",
    "y_test = to_categorical(y_test, num_classes=10)\n",
    "print(f'After one-hot encoding: {y_train[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Scikit-Learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original label: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "One-hot encoded label: [1. 0.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Reshape y for one-hot encoding (it needs to be a 2D array)\n",
    "y_train_reshaped = np.array(y_train).reshape(-1, 1)\n",
    "y_test_reshaped = np.array(y_test).reshape(-1, 1)\n",
    "\n",
    "# One-hot encode the labels\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_train_encoded = encoder.fit_transform(y_train_reshaped)\n",
    "y_test_encoded = encoder.transform(y_test_reshaped)\n",
    "\n",
    "# Print the first example\n",
    "print(\"Original label:\", y_train[0])\n",
    "print(\"One-hot encoded label:\", y_train_encoded[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Forward Neural Networks with Keras\n",
    "\n",
    "Now that we have prepared our data, it's time to build a simple neural network! In this section, we will use the Keras API to build a simple feed forward neural network. We will then train the model on the MNIST dataset, and evaluate its performance on the test set.\n",
    "\n",
    "In most modern deep learning frameworks, the process of building a model can be broken down into a few steps:\n",
    "\n",
    "- Define the model architecture: this is where we define the layers of the model, and how they are connected to each other.\n",
    "- Compile the model: this is where we define the loss function, the optimizer, and the metrics that we want to use to evaluate the model.\n",
    "- Train the model: this is where we train the model on the training data.\n",
    "\n",
    "Let's start with defining the model architecture. There are two ways to do this in Keras - the Sequential API and the Functional API. The Sequential API is the simplest way to build a model, and is suitable for most use cases. The Functional API is more flexible, and allows you to build more complex models. We will start with the Sequential API, and then we will look at the Functional API later in the course.\n",
    "\n",
    "Our simple neural network will be \"fully-connected\". This means that each neuron in a given layer is connected to every neuron in the next layer. This is also known as a \"dense\" layer. We will use the `Dense` class from Keras to define our layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nrojas/anaconda3/envs/dsi_participant/lib/python3.9/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m8,320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m650\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">17,226</span> (67.29 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m17,226\u001b[0m (67.29 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">17,226</span> (67.29 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m17,226\u001b[0m (67.29 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Define the model using the Sequential API\n",
    "model = Sequential([\n",
    "    # Input layer (implicitly handled by the first Dense layer)\n",
    "    Dense(128, activation='relu', input_shape=(64,)),  # 128 neurons, ReLU activation\n",
    "    Dense(64, activation='relu'),  # 64 neurons, ReLU activation\n",
    "    Dense(10, activation='softmax')  # Output layer: 10 neurons, Softmax activation\n",
    "])\n",
    "\n",
    "# Print a summary of the model architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Activation Functions: ReLU and Softmax**\n",
    "\n",
    "---\n",
    "\n",
    "## **What is ReLU?**\n",
    "\n",
    "### **Definition**\n",
    "ReLU (Rectified Linear Unit) is an activation function used in neural networks. It is defined as:\n",
    "\\[\n",
    "f(x) = \\max(0, x)\n",
    "\\]\n",
    "- If \\(x > 0\\), the output is \\(x\\).\n",
    "- If \\(x \\leq 0\\), the output is \\(0\\).\n",
    "\n",
    "---\n",
    "\n",
    "### **Why Use ReLU?**\n",
    "1. **Adds Non-Linearity**:\n",
    "   - Enables the network to learn complex, non-linear patterns.\n",
    "   - Without it, the network would behave like a simple linear model, no matter how deep.\n",
    "\n",
    "2. **Efficient Computation**:\n",
    "   - Simple and fast to compute (\\(x > 0\\)).\n",
    "\n",
    "3. **Improves Training**:\n",
    "   - Avoids the **vanishing gradient problem** for positive inputs, helping deeper networks train effectively.\n",
    "\n",
    "---\n",
    "\n",
    "### **Where is ReLU Used?**\n",
    "- Used in the **hidden layers** of neural networks.\n",
    "- Helps process and extract features from input data.\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "## **What is Softmax?**\n",
    "\n",
    "### **Definition**\n",
    "Softmax is an activation function used in the **output layer** of neural networks for **multi-class classification**. It converts raw model outputs (logits) into probabilities.\n",
    "\n",
    "The formula for softmax is:\n",
    "\\[\n",
    "\\sigma(z_i) = \\frac{e^{z_i}}{\\sum_{j} e^{z_j}}\n",
    "\\]\n",
    "Where:\n",
    "- \\(z_i\\): Input (logit) for class \\(i\\).\n",
    "- \\(e^{z_i}\\): Exponential of the input for class \\(i\\).\n",
    "- \\(\\sum_{j} e^{z_j}\\): Sum of exponentials for all classes.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why Use Softmax?**\n",
    "1. **Probabilistic Interpretation**:\n",
    "   - Outputs are interpretable as probabilities (values between 0 and 1, summing to 1).\n",
    "\n",
    "2. **Multi-Class Classification**:\n",
    "   - Handles tasks where each input belongs to one of several **mutually exclusive classes**.\n",
    "\n",
    "3. **Works with Categorical Crossentropy**:\n",
    "   - Softmax pairs perfectly with this loss function for classification tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### **Where is Softmax Used?**\n",
    "- Used in the **output layer** of neural networks for tasks like digit recognition, image classification, etc.\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "## **Comparison: ReLU vs. Softmax**\n",
    "\n",
    "| **Aspect**           | **ReLU**                           | **Softmax**                          |\n",
    "|-----------------------|-------------------------------------|---------------------------------------|\n",
    "| **What it does**      | Passes positive inputs, zeros out negatives | Converts raw outputs into probabilities |\n",
    "| **Where it’s used**   | Hidden layers                     | Output layer (classification)         |\n",
    "| **Purpose**           | Adds non-linearity to the network | Ensures output interpretable as probabilities |\n",
    "| **Output Range**      | [0, ∞)                             | [0, 1]                               |\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "- **ReLU** is vital for hidden layers as it enables the network to learn non-linear patterns efficiently.\n",
    "- **Softmax** is crucial for the output layer in multi-class classification problems, producing interpretable probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Congratulations! You have just built your first neural network with Keras. As we can confirm from the `model.summary()` output, our model has 3 layers. The first layer has 64 neurons, the second layer has 64 neurons, and the output layer has 10 neurons. The output layer uses the softmax activation function, which is commonly used for multi-class classification problems. The other layers use the ReLU activation function, which is commonly used for hidden layers in neural networks.\n",
    "\n",
    "Next, we need to compile the model. This is where we define the loss function, the optimizer, and the metrics that we want to use to evaluate the model. We will use the `compile` method of the model to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Compiling a Model in Keras**\n",
    "\n",
    "---\n",
    "\n",
    "## **What is Compiling a Model?**\n",
    "\n",
    "Compiling a model in Keras configures it for training by linking three key components:\n",
    "1. **Loss Function**: Quantifies the error between the model's predictions and the true labels. The model minimizes this during training.\n",
    "2. **Optimizer**: Determines how the model updates its weights to minimize the loss.\n",
    "3. **Metrics**: Specifies additional evaluation criteria to monitor during training and testing.\n",
    "\n",
    "---\n",
    "\n",
    "## **Hyperparameters for Compilation**\n",
    "\n",
    "### **1. Loss Function**\n",
    "The loss function depends on the type of problem:\n",
    "- **Categorical Crossentropy** (for multi-class classification):\n",
    "  \\[\n",
    "  \\text{Loss} = -\\sum_{i} y_i \\log(\\hat{y}_i)\n",
    "  \\]\n",
    "  - \\(y_i\\): True probability (one-hot encoded).\n",
    "  - \\(\\hat{y}_i\\): Predicted probability.\n",
    "- **Mean Squared Error (MSE)** (for regression): Measures the average squared difference between predicted and true values.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Optimizer**\n",
    "The optimizer adjusts model weights to minimize the loss function. Common optimizers:\n",
    "- **SGD (Stochastic Gradient Descent)**: Updates weights using the gradient of the loss.  \n",
    "- **Adam (Adaptive Moment Estimation)**: Combines momentum and RMSProp optimizers for faster convergence.\n",
    "\n",
    "#### Key Parameters:\n",
    "- **Learning Rate (\\(\\alpha\\))**: Controls the step size for updates.\n",
    "- **Beta1, Beta2** (specific to Adam): Decay rates for moving averages of the gradient and squared gradient.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Metrics**\n",
    "Metrics provide additional monitoring criteria.  \n",
    "For classification:\n",
    "- **Accuracy**:\n",
    "  \\[\n",
    "  \\text{Accuracy} = \\frac{\\text{Correct Predictions}}{\\text{Total Samples}}\n",
    "  \\]\n",
    "For regression:\n",
    "- **Mean Absolute Error (MAE)**: Measures the average absolute difference between predictions and true values.\n",
    "\n",
    "---\n",
    "\n",
    "## **Example: Compiling the Model**\n",
    "The following code compiles the model:\n",
    "\n",
    "```python\n",
    "model.compile(\n",
    "    optimizer='adam',               # Optimizer for adjusting weights\n",
    "    loss='categorical_crossentropy', # Loss function for multi-class classification\n",
    "    metrics=['accuracy']            # Metric to monitor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='categorical_crossentropy', # Loss function\n",
    "    optimizer='adam', # Optimizer\n",
    "    metrics=['accuracy'] # Metrics to evaluate the model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Because we are predicting which class a sample belongs to, we will use the `categorical_crossentropy` function. This loss function is commonly used for multi-class classification problems. \n",
    "\n",
    "For our optimizer, we are using the standard stochastic gradient descent (SGD) algorithm. This is a simple optimizer that works well for many problems. We will look at more advanced optimizers later in the course.\n",
    "\n",
    "Finally, we are using the `accuracy` metric to evaluate the model. This is a common metric for classification problems, and it is simply the fraction of samples that are correctly classified. This is an easier metric for us to understand, but it's not quite as useful for actually training the model (for example, it doesn't tell us how \"confident\" the model is in its predictions).\n",
    "\n",
    "Now that we have (a) defined the model architecture and (b) compiled the model, we are ready to train the model. We will use the `fit` method of the model to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Training a Model in Keras**\n",
    "\n",
    "---\n",
    "\n",
    "## **What is Model Training (Fitting)?**\n",
    "\n",
    "The `fit()` function in Keras is used to train the model on the training data. During this process:\n",
    "1. The model adjusts its weights using the optimizer to minimize the loss function.\n",
    "2. Training is performed in **epochs**, where the model sees the entire dataset multiple times.\n",
    "3. Data is processed in **batches** to efficiently handle large datasets.\n",
    "\n",
    "---\n",
    "\n",
    "## **Key Parameters of `fit()`**\n",
    "\n",
    "### **1. Training Data**\n",
    "- **`x`**: Input features (training data).\n",
    "- **`y`**: Target labels (true output values).\n",
    "\n",
    "### **2. Validation Data**\n",
    "- Helps monitor the model's performance on unseen data during training.\n",
    "- Provided using the `validation_data` parameter (e.g., a tuple `(x_val, y_val)`).\n",
    "\n",
    "### **3. Epochs**\n",
    "- The number of times the model processes the entire training dataset.\n",
    "- Increasing epochs allows the model to learn more, but excessive epochs can lead to overfitting.\n",
    "\n",
    "### **4. Batch Size**\n",
    "- The number of samples processed at once during training.\n",
    "- Smaller batches:\n",
    "  - Provide more granular weight updates but are slower.\n",
    "- Larger batches:\n",
    "  - Provide faster updates but may miss finer details in the data.\n",
    "\n",
    "### **5. Verbosity**\n",
    "- Controls the output displayed during training.\n",
    "  - `0`: Silent.\n",
    "  - `1`: Progress bar.\n",
    "  - `2`: One line per epoch.\n",
    "\n",
    "---\n",
    "\n",
    "## **Example: Training the Model**\n",
    "\n",
    "### **Code to Train the Model**\n",
    "```python\n",
    "history = model.fit(\n",
    "    x_train,             # Input features (training data)\n",
    "    y_train,             # Target labels\n",
    "    validation_data=(x_test, y_test),  # Validation data\n",
    "    epochs=10,           # Number of times the dataset is processed\n",
    "    batch_size=32,       # Number of samples per batch\n",
    "    verbose=1            # Display progress bar\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-27 16:28:53.975160: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - accuracy: 0.3165 - loss: 3.0270 - val_accuracy: 0.8299 - val_loss: 0.6275\n",
      "Epoch 2/5\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8992 - loss: 0.3619 - val_accuracy: 0.9062 - val_loss: 0.3336\n",
      "Epoch 3/5\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9593 - loss: 0.1689 - val_accuracy: 0.9271 - val_loss: 0.2569\n",
      "Epoch 4/5\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9744 - loss: 0.1004 - val_accuracy: 0.9306 - val_loss: 0.2308\n",
      "Epoch 5/5\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9865 - loss: 0.0663 - val_accuracy: 0.9306 - val_loss: 0.2220\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x17373b580>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train, # Training data\n",
    "    y_train, # Training labels\n",
    "    epochs=5, # Number of epochs\n",
    "    batch_size=32, # Number of samples per batch\n",
    "    validation_split=0.2 # Use 20% of the data for validation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We have now trained our model! We can see that the model has been trained for 5 epochs, and the loss and accuracy have been printed for each epoch. We can also see that the model has been evaluated on the validation data at the end of each epoch. This is useful for us to see how the model is performing on data that it hasn't seen during training.\n",
    "\n",
    "Once the model is trained, it's time to evaluate the model on the test set. We can use the `evaluate` method of the model to do this. If you were building a model for a real-world application, this is the very last thing you would do, and the result here would be the figure you'd report in your paper or presentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9352 - loss: 0.1533\n",
      "Loss:     0.11\n",
      "Accuracy: 95.83%\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "print(f'Loss:     {loss:.2f}')\n",
    "print(f'Accuracy: {accuracy*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Hopefully you have achieved an accuracy of around 95%. This is pretty good, but we can do better! In the next section, we will look at how we can improve the performance of our model by using a more advanced optimizer. But before we get there, let's do one other thing - let's look at the predictions that our model is making on the test set. When you are building a model, it's often useful to have a look at some of the examples your model is getting wrong. Sometimes this can reveal problems with the data, or it can give you ideas for how to improve your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the predictions for the test data\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Get the index of the largest probability (i.e. the predicted class)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "true_classes = np.argmax(y_test, axis=1)\n",
    "misclassified_indices = np.where(predicted_classes != true_classes)[0]\n",
    "\n",
    "# Get the misclassified samples themselves\n",
    "misclassified_samples = X_test[misclassified_indices]\n",
    "misclassified_labels = np.argmax(y_test[misclassified_indices], axis=1)\n",
    "\n",
    "# Pick 9 random misclassified samples\n",
    "random_indices = np.random.choice(len(misclassified_indices), 9, replace=False)\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(6, 6))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(misclassified_samples[random_indices[i]].reshape(8, 8), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    ax.set_title(f\"Pred: {predicted_classes[misclassified_indices[random_indices[i]]]}, Real: {misclassified_labels[random_indices[i]]}\")\n",
    "\n",
    "    # Removing axis labels\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "What do you think? Would you have made the same mistakes as the model? Determining whether the mistakes are \"understandable\" is a rough way of seeing if you could improve the model further, or if this is the best you can do with the data you have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Exercises: Impact of the Optimizer\n",
    "\n",
    "In this section, you will play around with the optimizer and see how it affects the performance of the model. We will start with the standard SGD optimizer, and then we will look at more advanced optimizers.\n",
    "\n",
    "1. Try decreasing the learning rate of the SGD optimizer by a factor of 10, or 100. What do you observe?\n",
    "2. Try increasing the learning rate of the SGD optimizer. What happens?\n",
    "3. The SGD optimizer has a momentum parameter. In a nutshell, this parameter controls how much the gradient from the previous step affects the current step. Try enabling momentum in the SGD optimizer with a value of 0.9. What happens?\n",
    "  \n",
    "**Notes**: \n",
    "\n",
    "The keras API documentation is available at:\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras\n",
    "\n",
    "It is also possible to learn more about the parameters of a class by using the question mark: type and evaluate:\n",
    "\n",
    "```python\n",
    "optimizers.SGD?\n",
    "```\n",
    "\n",
    "in a jupyter notebook cell.\n",
    "\n",
    "It is also possible to type the beginning of a function call / constructor and type \"shift-tab\" after the opening paren:\n",
    "\n",
    "```python\n",
    "optimizers.SGD(<shift-tab>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here - copy the relevant parts from the previous section and add more cells as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's try a more advanced optimizer. Adam is likely the most popular optimizer for deep learning. It is an adaptive learning rate optimizer, which means that it automatically adjusts the learning rate based on how the training is going. This can be very useful, as it means that we don't need to manually tune the learning rate. Let's see how it performs on our model.\n",
    "\n",
    "\n",
    "1. Replace the SGD optimizer by the Adam optimizer from keras and run it\n",
    "  with the default parameters.\n",
    "\n",
    "2. Add another hidden layer with ReLU activation and 64 neurons. Does it improve the model performance?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises: Forward Pass and Generalization\n",
    "\n",
    "Let's look in more detail at how the model makes predictions on the test set. We will walk through each step of making predictions, examining exactly what's going on.\n",
    "\n",
    "To start, we will apply our model to the test set, and look at what we get as output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_tf = model(X_test)\n",
    "predictions_tf[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(predictions_tf), predictions_tf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw output of the model is a tensor of shape `(360, 10)`. This means that we have 360 samples, and for each sample we have 10 values. Each of these values represents the probability that the sample belongs to a given class. This means that we have 10 probabilities for each sample, and the sum of these probabilities is 1. We can confirm this by summing the probabilities for each sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.reduce_sum(predictions_tf, axis=1)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "...okay, there might be a small rounding error here and there. This is to do with how floating point numbers are represented in computers, and it's not something we need to worry about for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also extract the label with the highest probability using the tensorflow API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels_tf = tf.argmax(predictions_tf, axis=1)\n",
    "predicted_labels_tf[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "One helpful aspect of this approach is that we don't just get the prediction, but also a sense of how confident the model is in its prediction. To see this in practice, let's take a look at some of the predictions the model is highly confident about (i.e. a lot of the probability mass is on one class):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the values corresponding to the predicted labels for each sample\n",
    "predicted_values_tf = tf.reduce_max(predictions_tf, axis=1)\n",
    "\n",
    "# Get the indices of the samples with the highest predicted values\n",
    "most_confident_indices_tf = tf.argsort(predicted_values_tf, direction='DESCENDING').numpy()[:9]\n",
    "\n",
    "# Get the 9 most confident samples\n",
    "most_confident_samples_tf = X_test[most_confident_indices_tf]\n",
    "\n",
    "# Get the true labels for the 9 most confident samples\n",
    "most_confident_labels_tf = np.argmax(y_test[most_confident_indices_tf], axis=1)\n",
    "\n",
    "# Plot the 9 most confident samples\n",
    "fig, axes = plt.subplots(3, 3, figsize=(6, 6))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(most_confident_samples_tf[i].reshape(8, 8), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    ax.set_title(f\"{most_confident_labels_tf[i]}\")\n",
    "\n",
    "    # Removing axis labels\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impact of Initialization\n",
    "\n",
    "Let's study the impact of a bad initialization when training\n",
    "a deep feed forward network.\n",
    "\n",
    "By default, Keras dense layers use the \"Glorot Uniform\" initialization\n",
    "strategy to initialize the weight matrices:\n",
    "\n",
    "- each weight coefficient is randomly sampled from [-scale, scale]\n",
    "- scale is proportional to $\\frac{1}{\\sqrt{n_{in} + n_{out}}}$\n",
    "\n",
    "This strategy is known to work well to initialize deep neural networks\n",
    "with \"tanh\" or \"relu\" activation functions and then trained with\n",
    "standard SGD.\n",
    "\n",
    "To assess the impact of initialization let us plug an alternative init\n",
    "scheme into a 2 hidden layers networks with \"tanh\" activations.\n",
    "For the sake of the example let's use normal distributed weights\n",
    "with a manually adjustable scale (standard deviation) and see the\n",
    "impact the scale value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "input_dim = 64\n",
    "hidden_dim = 64\n",
    "output_dim = 10\n",
    "\n",
    "normal_init = initializers.TruncatedNormal(stddev=0.01, seed=42)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(hidden_dim, input_dim=input_dim, activation=\"tanh\",\n",
    "                kernel_initializer=normal_init))\n",
    "model.add(Dense(hidden_dim, activation=\"tanh\",\n",
    "                kernel_initializer=normal_init))\n",
    "model.add(Dense(output_dim, activation=\"softmax\",\n",
    "                kernel_initializer=normal_init))\n",
    "\n",
    "model.compile(optimizer=optimizers.SGD(learning_rate=0.1),\n",
    "              loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the parameters of the first layer after initialization but before any training has happened:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = model.layers[0].weights[0].numpy()\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = model.layers[0].weights[1].numpy()\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=15, batch_size=32)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(history.history['loss'], label=\"Truncated Normal init\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model has been fit, the weights have been updated and notably the biases are no longer 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions:\n",
    "\n",
    "- Try the following initialization schemes and see whether\n",
    "  the SGD algorithm can successfully train the network or\n",
    "  not:\n",
    "  \n",
    "  - a very small e.g. `stddev=1e-3`\n",
    "  - a larger scale e.g. `stddev=1` or `10`\n",
    "  - initialize all weights to 0 (constant initialization)\n",
    "  \n",
    "- What do you observe? Can you find an explanation for those\n",
    "  outcomes?\n",
    "\n",
    "- Are more advanced solvers such as SGD with momentum or Adam able\n",
    "  to deal better with such bad initializations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "dsi_participant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
